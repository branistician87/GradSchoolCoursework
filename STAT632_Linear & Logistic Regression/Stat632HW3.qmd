---
title: "Stat632 HW 3"
author: "Brandon Keck"
format: pdf
editor: visual
---

```{r, warning = FALSE, message = FALSE}
# Load the necessary libraries
library(ISLR) # ISLR package to access the Auto dataset
library(ggplot2)
library(dplyr)
```

```{r}
data(Auto)
# head(Auto)
```

## Exercise 1.

### (a)

```{r}
ggplot(Auto, aes(horsepower, mpg)) +
  geom_point() +
  labs(title = "MPG vs Horsepower",
       x = "Horsepower",
       y = "MPG") +
  theme_minimal()
```

### (b)

$$
Y = \beta_0 + \beta_1x + \beta_2 x^2 + \epsilon 
$$

Where:

Y = mpg

X = horsepower

```{r}
lm_auto <- lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
summary(lm_auto)
```

$\hat{mpg} = 56.90 - 0.4662(horsepower) + 0.00123(horsepower^2)$

Each additional 1-unit increase in horsepower decreases mpg by 0.466 mpg on average. All predictors have p-values <2e-16 meaning they are highly significant.

### (c)

$$
\hat{mpg} = 56.90 - 0.4662(hosepower) + 0.00123(horsepower^2) 
$$


```{r}
56.9000997 - 0.4661896 * (150) + 0.0012305 * (150^2)
```


Here I am making a prediction by hand.It looks like if a vehicle was to have a horsepower of 150 we should see mpg be about 14.66. Now let's double check with the prediction function in R to see how close we are.


```{r}
pred <- predict(lm_auto, data.frame(horsepower = 150),
                interval = "prediction")
pred
```

From the regression model $$
\hat{mpg} = 56.90 - 0.4662(hosepower) + 0.00123(horsepower^2) 
$$ we predict that a vehicle with 150 horsepower will have an estimated mpg 14.66. Additionally, using a 95% prediction interval, we estimate that the actual mpg could fall between 6.03 and 23.29, accounting for variability in individual observations.

### (d)

Next we plot the estimated regression curve on the scatterplot.

```{r}
ggplot(data = Auto, aes(horsepower, mpg)) +
  geom_point(size = 1) +
  stat_smooth(method = 'lm', formula = y ~ poly(x, 2), se = TRUE) +
  labs(title = "Quadratic Regression: MPG vs HP",
       x = "Horsepower",
       y = "MPG") +
  theme_minimal()
```

### (e)

Plot of the residuals vs fitted values, and a QQ plot of the Standardized Residuals

```{r}
# residuals vs fitted values
par(mfrow = c(1, 2))
plot(predict(lm_auto), resid(lm_auto),
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

# Q-Q plot
qqnorm(rstandard(lm_auto))
qqline(rstandard(lm_auto))
```




The Residuals vs Fitted plot suggests that variance might not be perfectly equal across all fitted values. Observations 334, 323, and 155 stand out as potentially influential points. However, there is no extreme funnel shape, meaning heteroscedasticity is not a major issue.

The Normal Q-Q plot shows that most residuals follow the theoretical normal distribution, indicating approximate normality. However, there is some deviation in the tails, suggesting potential outliers or slight skewness.

Overall, while there are minor concerns with variance and influential points, the model does not show any severe violations of assumptions.

## Exercise 2:

```{r}
data("Carseats")
# head(Carseats)
```

### (a)

```{r}
lm_seats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(lm_seats)
```

$\hat{Sales} = 13.04 - 0.544(Price) - 0.0219(Urban) + 1.2006(US)$

For each \$1 increase in Price decreases Sales by 0.0544 cents.

The coefficient for Urban is not significant. It has a high p-value of 0.936 which is greater than our significance value of $\alpha = 0.05$. Meaning there is no evidence that being in an urban area affects Sales.

### (b)

$\hat{Sales} = 13.04 - 0.544(Price) - 0.0219(Urban) + 1.2006(US)$

1.  Intercept $(\beta_0 = 13.04)$

-   When Price = 0, Urban = No, and Us = No, the predicted Sales = 13.03

-   Not practically meaningful because a price of 0 is unrealistic.

2.  Price $(\beta_1 = 0.0544)$

-   For each \$1 increase in Price, Sales decreases by 0.0544 cents on average.

3.  Urban $(\beta_2 = -0.0219)$

-   If a store is in an Urban area, Sales are lower by 0.0219 cents compared to a non-urban area.

4.  US $(\beta_3 = 1.2006)$

-   If a store is in the US, Sales are higher by 1.20 units compared to a store outside the US.

### (c)

$$
\hat{Sales} = 13.04 - 0.0544(Price) - 0.0219(Urban) + 1.2006(US)
$$

### (d)

$H_0: \beta_j = 0$

From the summary of the multiple linear regression model we can reject the $H_0$ for Price and US. The output shows that these predictors have p-values very close to 0, which means that the $\beta$ coefficients are significantly different from 0. From the model so far only Price and US are useful predictors of Sales. Urban has a p-value of 0.936 and we cannot reject $H_0$ meaning it does not significantly affect Sales and can be removed from the model.

### (e)

```{r}
lm_seats2 <- lm(Sales ~ Price + US, data = Carseats)
summary(lm_seats2)
```

After removing the predictor variable Urban from the model we see a slightly improved Adjusted R-squared from 0.2335 to 0.2354 confirming that the Urban predictor was unnecessary. The residual standard error also slightly decreased, suggesting a better fit model.

### (f)

Comparing the models from part (a) and part (e) we find that the reduced model in part (e) provides a slightly better fit. The Adjusted R-squared increased slightly, indicating that the reduced model is more efficient without sacrificing explanatory power. Additionally, the residual standard error decreased, suggesting a modestly improved fit. However, the most compelling evidence for the superiority of the reduced model is the higher F-statistic, which indicated a stronger overall model.

### (g)

```{r}
confint(lm_seats2)
```

We calculated a 95% confidence interval for the coefficients in the reduced model. The results confirm that both Price and US are statistically significant predictors of Sales as their confidence intervals do not include 0.

Price: The interval (-0.0648, -0.0442) suggests that for every $1 increase in Price, Sales decreases by approximately 0.044 to 0.065 units.

US: The interval (0.6915, 1.7078) indicates that being in the US increases Sales by approximately 0.69 to 1.71 units on average.


